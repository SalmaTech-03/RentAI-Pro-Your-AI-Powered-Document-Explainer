# Use a standard Python 3.10 base image
FROM python:3.10-slim

# Set the working directory for the application
WORKDIR /app

# Install system dependencies and the official Ollama script
RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*
RUN curl -fsSL https://ollama.com/install.sh | sh

# Copy all your project files into the container
COPY . .

# Install all the Python libraries from requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Set the home directory to a writeable location to avoid permission errors
ENV HOME=/tmp
ENV OLLAMA_MODELS=/tmp/ollama_models

# Expose the port our production server will run on
EXPOSE 7860

# The command that will be executed when the container starts.
# It starts Ollama, pulls models, and then starts the Gunicorn server WITH A LONGER TIMEOUT.
CMD ["sh", "-c", "ollama serve & sleep 10 && ollama pull nomic-embed-text && ollama pull tinyllama && gunicorn --bind 0.0.0.0:7860 --workers 2 --timeout 300 backend:app"]